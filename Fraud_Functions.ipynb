{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Credit Card Fraud Standalone Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(test_size,val_size,x_features,y_target):\n",
    "    val_percent = val_size / (1 - test_size)\n",
    "    X_trainVal, X_test, y_trainVal, y_test = train_test_split(x_features, y_target, test_size=test_size, random_state=42, stratify=y_target)\n",
    "#     X_train, X_val, y_train, y_val = train_test_split(X_trainVal, y_trainVal, test_size=val_percent, random_state=42, stratify=y_trainVal)\n",
    "    \n",
    "    return X_test,y_test,X_trainVal,y_trainVal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cv_learning_curve(x_data,y_data,folds,penalty,step):\n",
    "    # Include time in the features\n",
    "#     ffeats_time = raw_data[raw_data.columns[0:-1]]\n",
    "#     target_time = raw_data[raw_data.columns[-1]]\n",
    "\n",
    "#     X_train,y_train,X_val,y_val,X_test,y_test,X_trainVal_t,y_trainVal_t = split_data(test_size=0.2,val_size=0.2,x_features=ffeats_time,y_target=target_time)\n",
    "\n",
    "    learning_stats = []\n",
    "    clf1 = LogisticRegression(random_state=0,penalty='l2',C=penalty,solver='lbfgs')\n",
    "\n",
    "    for size in np.arange(step,1.00,step):\n",
    "        clear_output()\n",
    "        display(\"Cross Validation Test {} of {}...\".format(int(round(size*100)), int(1/step))) \n",
    "\n",
    "        X_cv, X_unused, y_cv, y_unused = train_test_split(x_data, y_data, test_size=1-size, random_state=42, stratify=y_data)\n",
    "        scores = cross_validate(clf1, X_cv, y_cv, cv=folds, scoring='f1', return_train_score=True)\n",
    "        learning_stats.append({'data_size':size*100,'train_score':np.mean(scores['train_score']),'test_score':np.mean(scores['test_score'])})\n",
    "\n",
    "    clear_output()\n",
    "    display(\"Cross Validation Test {} of {}...\".format(int(1/step), int(1/step)))\n",
    "    scores = cross_validate(clf1, x_data, y_data, cv=folds, scoring='f1', return_train_score=True)\n",
    "    learning_stats.append({'data_size':100,'train_score':np.mean(scores['train_score']),'test_score':np.mean(scores['test_score'])})\n",
    "    \n",
    "    learning_df = pd.DataFrame(learning_stats)\n",
    "\n",
    "    fig, ax1 = plt.subplots()\n",
    "    fig.set_size_inches(15,10)\n",
    "\n",
    "    ax1.plot(learning_df['data_size'],learning_df['train_score'],color='red')\n",
    "    ax1.plot(learning_df['data_size'],learning_df['test_score'],color='green')\n",
    "    ax1.set_ylabel('F1 Score')\n",
    "    ax1.set_xlabel('Data Set Percentage of Total Training Validation Set')\n",
    "    ax1.set_title('Learning Curve')\n",
    "#     ssax1.xlim(0,100)\n",
    "\n",
    "    fig.legend(['Train Score','Validation Score'],loc=(0.75, 0.82), prop={'size':12})\n",
    "\n",
    "    ax1.grid()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def polynomial_test(x_data,y_data,folds,degree_list,reg_list):\n",
    "\n",
    "    poly_scores = []\n",
    "    \n",
    "    log_reg_params = {\"degree\":degree_list,\"penalty\": ['l2'], 'C': reg_list}\n",
    "    grid = ParameterGrid(log_reg_params)\n",
    "    count = 1\n",
    "\n",
    "    for params in grid:\n",
    "\n",
    "        clear_output()\n",
    "        print(f\"Polynomial test {count} of {len(grid)}...\") \n",
    "        count += 1\n",
    "        \n",
    "        clf = LogisticRegression(random_state=0,penalty='l2',C=params['C'],solver='lbfgs')\n",
    "        poly = PolynomialFeatures(degree = params['degree'], interaction_only=False, include_bias=True)\n",
    "        X_poly = poly.fit_transform(x_data)\n",
    "        scores = cross_validate(clf, X_poly, y_data, cv=folds, scoring='f1', return_train_score=True)\n",
    "\n",
    "        poly_scores.append({'Polynomial_Degree':params['degree'],'Regularize':params['C'], \\\n",
    "                            'train_score':np.mean(scores['train_score']),'test_score':np.mean(scores['test_score'])})\n",
    "\n",
    "    poly_results = pd.DataFrame(poly_scores)\n",
    "    display(poly_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cv_test(x_data,y_data,folds,regularize,penalty, drop):\n",
    "    \n",
    "    strat_scores = []\n",
    "    skf = StratifiedKFold(n_splits=folds, random_state=0, shuffle=False)\n",
    "    skf.get_n_splits(x_data, y_data)\n",
    "    count = 1\n",
    "\n",
    "    for train_index, test_index in skf.split(x_data, y_data):\n",
    "#         clear_output()\n",
    "#         print(\"Cross Validation Fold {} of {}...\".format(count, folds))\n",
    "        count += 1\n",
    "\n",
    "        clf2 = LogisticRegression(random_state=0,penalty=penalty,C=regularize,solver='lbfgs').fit(x_data.iloc[train_index],y_data.iloc[train_index])\n",
    "\n",
    "        y_prob_train = clf2.predict_proba(x_data.iloc[train_index])\n",
    "        y_prob_test = clf2.predict_proba(x_data.iloc[test_index])\n",
    "\n",
    "        y_pred_train = np.array(y_prob_train[:,1] > 0.5).astype(int)\n",
    "        y_pred_test = np.array(y_prob_test[:,1] > 0.5).astype(int)\n",
    "\n",
    "    #     plot_pr_curve(y_trainVal.iloc[test_index], y_prob_test[:,1])\n",
    "\n",
    "        train_result = precision_recall_fscore_support(y_data.iloc[train_index], y_pred_train, average='binary',pos_label=1)\n",
    "        test_result = precision_recall_fscore_support(y_data.iloc[test_index], y_pred_test, average='binary',pos_label=1)\n",
    "\n",
    "        strat_scores.append({'Train_Score':train_result[2],'Train_Recall':train_result[1], 'Train_Precision':train_result[0],\n",
    "        'Test_Score':test_result[2], 'Test_Recall':test_result[1], 'Test_Precision':test_result[0],'Regularization':regularize,\n",
    "        'Penalty':penalty, 'Drop':drop})\n",
    "        \n",
    "    scores = pd.DataFrame(strat_scores)\n",
    "\n",
    "    return scores,pd.DataFrame(scores.mean()).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_importance(x_data,y_data,folds):\n",
    "    \n",
    "    strat_scores = []\n",
    "    cv_results = []\n",
    "    skf = StratifiedKFold(n_splits=folds, random_state=0, shuffle=False)\n",
    "    skf.get_n_splits(x_data, y_trainVal)\n",
    "    count = 1\n",
    "\n",
    "    for train_index, test_index in skf.split(x_data, y_data):\n",
    "        \n",
    "        clf2 = LogisticRegression(random_state=0,penalty='l2',C=0.01,solver='lbfgs').fit(x_data.iloc[train_index],y_data.iloc[train_index])\n",
    "        \n",
    "        for col in x_data.columns:\n",
    "\n",
    "            X = x_data.iloc[test_index].copy()\n",
    "            X[col] = np.random.permutation(X[col].values)\n",
    "\n",
    "            y_prob_test = clf2.predict_proba(X)\n",
    "            y_pred_test = np.array(y_prob_test[:,1] > 0.5).astype(int)\n",
    "\n",
    "            test_result = precision_recall_fscore_support(y_data.iloc[test_index], y_pred_test, average='binary',pos_label=1)\n",
    "            strat_scores.append({'Test_Score':test_result[2], 'Test_Recall':test_result[1], 'Test_Precision':test_result[0],'Column':col})\n",
    "\n",
    "        cv_results.append(pd.DataFrame(strat_scores))\n",
    "        strat_scores = []\n",
    "\n",
    "#   compute average scores for each cv run \n",
    "    cv_results_total = (reduce(lambda x, y: x.add(y, fill_value=0), cv_results))/(len(cv_results))\n",
    "    cv_sorted = cv_results_total.sort_values(by = 'Test_Score', ascending= False, axis = 0).head(10)\n",
    "    cv_sorted.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    return cv_sorted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feat_select_recursive_scrambling(x_data,y_data,runs,cv_folds):\n",
    "\n",
    "    dropped_columns = []\n",
    "    results = pd.DataFrame()\n",
    "    count = 1\n",
    "\n",
    "    X_poly = data.X_poly_trainVal\n",
    "    y_trainVal = data.y_trainVal\n",
    "\n",
    "    for run in range(runs):\n",
    "\n",
    "        clear_output()\n",
    "        print(f\"Recursive Scrambling Test {count} of {runs}...\")\n",
    "\n",
    "#       return dataframe of the scores when each column is scrambled  \n",
    "        result = feature_importance(x_data,y_data,folds=cv_folds)\n",
    "\n",
    "#       the highest scored iteration is first, so drop the scrambled column associated with this iteration  \n",
    "        col_to_drop = int(result['Column'].iloc[0])\n",
    "    \n",
    "#       append to the running list of dropped columns\n",
    "        dropped_columns.append(col_to_drop)\n",
    "    \n",
    "#       drop the column from the data set\n",
    "        x_data.drop(x_data.columns[col_to_drop],axis=1,inplace=True)\n",
    "\n",
    "#       conduct the CV test \n",
    "        total_results_df, results_df = cv_test(x_data, y_data, folds=cv_folds, regularize=0.01, penalty = 'l2',drop=['Fixed'])\n",
    "\n",
    "        dropped_run = dropped_columns.copy()\n",
    "        results_df['Dropped_Columns'] = [dropped_run]\n",
    "        results_df['Hypothetical_Score'] = result['Test_Score'].iloc[0]\n",
    "        results = pd.concat([results,results_df])\n",
    "        count += 1  \n",
    "        \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feat_select_mutual_info(x_data,y_data,fit,plot):\n",
    "    \n",
    "#   if fit is true, compute the feature importance using the mutual information criteria \n",
    "    if fit == True:\n",
    "        mu = mutual_info_classif(x_data,y_data)\n",
    "        pdb.set_trace()\n",
    "        mutual_info = pd.DataFrame(mu).reset_index(inplace=True,drop=True)\n",
    "        mutual_info.columns = ['index','feat_importance']\n",
    "        mutual_info.to_csv(\"mutual_info2.csv\")\n",
    "       \n",
    "#   if fit is false, read the previously computed mutual information feature results. If file has not been created, throw a warning message\n",
    "    if fit == False:\n",
    "        try:\n",
    "            mutual_info = pd.read_csv(\"mutual_info2.csv\").drop(['Unnamed: 0'],axis=1)\n",
    "        \n",
    "        except:\n",
    "            print(\"Mutual information feature selection has not yet been conducted. Please set 'fit=True'\")\n",
    "            return\n",
    "        \n",
    "# plot features in order of importance to visualize an intuitive cut off\n",
    "    if plot == True:\n",
    "\n",
    "        mi_sorted = mutual_info.sort_values(by=\"feat_importance\",ascending=False)\n",
    "        mi_sorted.reset_index(drop=True, inplace=True)\n",
    "        \n",
    "        fig, ax1 = plt.subplots()\n",
    "        fig.set_size_inches(20,5)\n",
    "\n",
    "        ax1.plot(mi_sorted.index[:-1],mi_sorted['feat_importance'].iloc[:-1],color='red')\n",
    "        \n",
    "        ax1.set_xlabel(\"Reordered Polynomial Feature Index\")\n",
    "        ax1.set_ylabel(\"Feature Importance\")\n",
    "        ax1.set_title(\"Mutual Information Feature Importance\")\n",
    "        \n",
    "        plt.xticks(np.arange(0, mi_sorted.shape[0]+1, 50))\n",
    "        ax1.set_xlim(0,mi_sorted.shape[0])\n",
    "        ax1.grid()\n",
    "        plt.show()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cv_mutual_info(x_data,y_data,reg_list,drop_list,cv_folds):\n",
    "\n",
    "#   read the mutual information feature importance from csv. If no file exists throw an error\n",
    "    try:\n",
    "        mi_df = pd.read_csv(\"mutual_info2.csv\")\n",
    "        \n",
    "    except:\n",
    "        print(\"Mutual Information feature selection has not been conducted, please run function 'feat_select_mutual_info' first.\")\n",
    "        return\n",
    "    \n",
    "#   sort by feature importance with the least importance features appearing first so they can be dropped\n",
    "    mi_sorted = mi_df.sort_values(by=\"feat_importance\",ascending=True)\n",
    "    mi_sorted.reset_index(drop=True, inplace=True)\n",
    "    mi_sorted.drop('Unnamed: 0',inplace=True,axis=1)\n",
    "\n",
    "#   define parameter list\n",
    "    results_mi = pd.DataFrame()\n",
    "    log_reg_params = {\"penalty\": ['l2'], 'C':reg_list, 'drop':drop_list}\n",
    "    grid = ParameterGrid(log_reg_params)\n",
    "    count = 1\n",
    "\n",
    "#   iterate over the parameter list\n",
    "    for params in grid:\n",
    "\n",
    "#       drop the top X least importance features  \n",
    "        x_data_dropped = x_data.drop(mi_sorted['index'].iloc[0:params['drop']].tolist(),axis=1)\n",
    "\n",
    "        clear_output()\n",
    "        print(f\"Test {count} of {len(grid)}...\")\n",
    "\n",
    "#       conduct cross validation with the filtered features\n",
    "        total_results_df, results_df = cv_test(x_data_dropped, y_data, folds=cv_folds, regularize=params['C'], penalty = params['penalty'], drop = params['drop'])\n",
    "        results_mi = pd.concat([results_mi,results_df])\n",
    "        count += 1\n",
    "\n",
    "    display(results_mi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feat_select_permutation_importance(x_data,y_data,fit,plot):\n",
    "    \n",
    "    #   if fit is true, compute the feature importance using the permutation importance criteria \n",
    "    if fit == True:\n",
    "        clf = LogisticRegression(random_state=0,penalty='l2',C=0.01,solver='lbfgs').fit(x_data, y_data)\n",
    "        pi = permutation_importance(estimator=clf,X=x_data,y=y_data)\n",
    "        perm_imp = pd.DataFrame(pi['Importances_Mean']).reset_index()\n",
    "        perm_imp.columns = ['index','Importances_Mean']\n",
    "        perm_imp.to_csv(\"permutation_imp.csv\")\n",
    "       \n",
    "#   if fit is false, read the previously computed permutation importance feature results. If file has not been created, throw a warning message\n",
    "    if fit == False:\n",
    "        try:\n",
    "            perm_imp = pd.read_csv(\"permutation_imp.csv\").drop(['Unnamed: 0'],axis=1)\n",
    "        \n",
    "        except:\n",
    "            print(\"Permutation Importance feature selection has not yet been conducted. Please set 'fit=True'\")\n",
    "            return\n",
    "        \n",
    "    if plot == True:\n",
    "        imp_count = Counter(perm_imp['Importances_Mean'] <= 0)\n",
    "        print(f\"There a {imp_count[1]} features with zero or negative importance and {imp_count[0]} features with a positive importance.\")\n",
    "        \n",
    "        perm_imp_filter = perm_imp[perm_imp['Importances_Mean'] > 0]\n",
    "        \n",
    "        perm_imp_sorted = perm_imp_filter.sort_values(by=\"Importances_Mean\", ascending=True)\n",
    "        perm_imp_sorted.reset_index(drop=True, inplace=True)\n",
    "        \n",
    "        fig, ax1 = plt.subplots()\n",
    "        fig.set_size_inches(20,5)\n",
    "\n",
    "        ax1.plot(perm_imp_sorted.index[:-1],perm_imp_sorted['Importances_Mean'].iloc[:-1],color='red')\n",
    "        \n",
    "        ax1.set_xlabel(\"Reordered Polynomial Feature Index\")\n",
    "        ax1.set_ylabel(\"Feature Importance\")\n",
    "        ax1.set_title(\"Permutation Feature Importance for Positive Importance Features\")\n",
    "        \n",
    "        plt.xticks(np.arange(0, perm_imp_sorted.shape[0]+1, 50))\n",
    "        ax1.set_xlim(0,perm_imp_sorted.shape[0])\n",
    "        ax1.grid()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cv_permutation_importance(x_data,y_data,reg_list,cv_folds,select,feat_num_filter):\n",
    "\n",
    "#   read the mutual information feature importance from csv. If no file exists throw an error\n",
    "    try:\n",
    "        permutation_df = pd.read_csv(\"permutation_imp.csv\")\n",
    "        \n",
    "    except:\n",
    "        print(\"Permutation Importance feature selection has not been conducted, please run function 'feat_select_permutation_importance' first.\")\n",
    "        return\n",
    "      \n",
    "    if select == 1:\n",
    "    #   filter out features with negative mean feature importance\n",
    "        permutation_cols_keep = list(permutation_df[permutation_df[\"Importances_Mean\"] > 0].index)\n",
    "        \n",
    "    elif select == 2:\n",
    "#         pdb.set_trace()\n",
    "        perm_imp_filter = permutation_df[permutation_df['Importances_Mean'] > 0]\n",
    "\n",
    "        perm_imp_sorted = perm_imp_filter.sort_values(by=\"Importances_Mean\", ascending=True)\n",
    "        perm_imp_sorted.reset_index(drop=False, inplace=True)\n",
    "        permutation_cols_keep = perm_imp_sorted['index'].tail(feat_num_filter).tolist()\n",
    "        \n",
    "\n",
    "#   define parameter list\n",
    "    results_perm = pd.DataFrame()\n",
    "    log_reg_params = {\"penalty\": ['l2'], 'C': reg_list, 'drop':['Fixed']}\n",
    "    grid = ParameterGrid(log_reg_params)\n",
    "    count = 1\n",
    "\n",
    "#   iterate over the parameter list\n",
    "    for params in grid:\n",
    "\n",
    "#       remove features with negative feature importance \n",
    "        x_data_dropped = x_data.loc[:,permutation_cols_keep]\n",
    "\n",
    "        clear_output()\n",
    "        print(f\"Test {count} of {len(grid)}...\")\n",
    "\n",
    "#       conduct cross validation with the filtered features\n",
    "        total_results_df, results_df = cv_test(x_data_dropped, y_data, folds=cv_folds, regularize=params['C'], penalty = params['penalty'], drop = params['drop'])\n",
    "        results_perm = pd.concat([results_perm,results_df])\n",
    "        count += 1\n",
    "\n",
    "    display(results_perm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feat_select_select_from_model(x_data,y_data,fit):\n",
    "    \n",
    "    #   if fit is true, compute the feature importance using the permutation importance criteria \n",
    "    if fit == True:\n",
    "        clf3 = LogisticRegression(random_state=0,penalty='l2',C=0.01,solver='lbfgs')\n",
    "        SFM = SelectFromModel(estimator=clf3).fit(x_data,y_data)\n",
    "        sfm_df = pd.DataFrame(list(SFM.get_support()))\n",
    "        sfm_df.to_csv(\"select_from_model.csv\")\n",
    "       \n",
    "#   if fit is false, read the previously computed SFM feature results. If file has not been created, throw a warning message\n",
    "    if fit == False:\n",
    "        try:\n",
    "            sfm_df = pd.read_csv(\"select_from_model.csv\")\n",
    "        \n",
    "        except:\n",
    "            print(\"Select from model feature selection has not yet been conducted. Please set 'fit=True'\")\n",
    "            return\n",
    "        \n",
    "    print(f\"The filtered data set has been reduced to {Counter(sfm_df['0'])[1]} from {x_data.shape[1]} features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cv_select_from_model(x_data,y_data,reg_list,cv_folds):\n",
    "\n",
    "#   read the mutual information feature importance from csv. If no file exists throw an error\n",
    "    try:\n",
    "        sfm_df = pd.read_csv(\"select_from_model.csv\")\n",
    "        sfm_cols_to_keep = list(sfm_df[\"0\"])\n",
    "        \n",
    "    except:\n",
    "        print(\"Select from model feature selection has not been conducted, please run function 'feat_select_select_from_model' first.\")\n",
    "        return\n",
    "\n",
    "    X_poly_filter = x_data.loc[:,sfm_cols_to_keep]\n",
    "\n",
    "    results_sfm = pd.DataFrame()\n",
    "    log_reg_params = {\"penalty\": ['l2'], 'C': reg_list, 'drop':['Fixed']}\n",
    "    grid = ParameterGrid(log_reg_params)\n",
    "    count = 1\n",
    "\n",
    "#   iterate over the parameter list\n",
    "    for params in grid:\n",
    "\n",
    "        clear_output()\n",
    "        print(f\"Test {count} of {len(grid)}...\")\n",
    "\n",
    "#       conduct cross validation with the filtered features\n",
    "        total_results_df, results_df = cv_test(X_poly_filter, y_data, folds=cv_folds, regularize=params['C'], penalty = params['penalty'], drop = params['drop'])\n",
    "        results_sfm = pd.concat([results_sfm,results_df])\n",
    "        count += 1\n",
    "\n",
    "    display(results_sfm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feat_select_variance_threshold(x_data,y_data,var_thresh,fit):\n",
    "    \n",
    "    #   if fit is true, compute the feature importance using the permutation importance criteria \n",
    "    if fit == True:\n",
    "        X_VT = VarianceThreshold(threshold = var_thresh).fit(x_data)\n",
    "        X_VarThresh = pd.DataFrame(X_VT.get_support())\n",
    "        X_VarThresh.columns = ['Column_Filter']\n",
    "        X_VarThresh.to_csv(\"var_threshold.csv\")\n",
    "       \n",
    "#   if fit is false, read the previously computed SFM feature results. If file has not been created, throw a warning message\n",
    "    if fit == False:\n",
    "        try:\n",
    "            X_VarThresh = pd.read_csv(\"var_threshold.csv\")\n",
    "            X_VarThresh.drop(\"Unnamed: 0\",axis=1,inplace=True)\n",
    "        \n",
    "        except:\n",
    "            print(\"Select from model feature selection has not yet been conducted. Please set 'fit=True'\")\n",
    "            return\n",
    "        \n",
    "    print(f\"The filtered data set has been reduced to {X_VarThresh[X_VarThresh['Column_Filter'] == True].shape[0]} from {x_data.shape[1]} features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cv_variance_threshold(x_data,y_data,reg_list,cv_folds):\n",
    "\n",
    "#   read the mutual information feature importance from csv. If no file exists throw an error\n",
    "    try:\n",
    "        X_VarThresh = pd.read_csv(\"var_threshold.csv\")\n",
    "        X_VarThresh.drop(\"Unnamed: 0\",axis=1,inplace=True)\n",
    "        \n",
    "    except:\n",
    "        print(\"Select from model feature selection has not been conducted, please run function 'feat_select_variance_threshold' first.\")\n",
    "        return\n",
    "\n",
    "    X_poly_filter = x_data.iloc[:,X_VarThresh.iloc[:,0].tolist()]\n",
    "    X_poly_filter.columns = [np.arange(0,X_poly_filter.shape[1],1)]\n",
    "\n",
    "    results_vs = pd.DataFrame()\n",
    "    log_reg_params = {\"penalty\": ['l2'], 'C': reg_list, 'drop':['Fixed']}\n",
    "    grid = ParameterGrid(log_reg_params)\n",
    "    count = 1\n",
    "\n",
    "#   iterate over the parameter list\n",
    "    for params in grid:\n",
    "\n",
    "        clear_output()\n",
    "        print(f\"Test {count} of {len(grid)}...\")\n",
    "\n",
    "#       conduct cross validation with the filtered features\n",
    "        total_results_df, results_df = cv_test(X_poly_filter, y_data, folds=cv_folds, regularize=params['C'], penalty = params['penalty'], drop = params['drop'])\n",
    "        results_vs = pd.concat([results_vs,results_df])\n",
    "        count += 1\n",
    "\n",
    "    display(results_vs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feat_select_RFECV(x_data,y_data,fit):\n",
    "    \n",
    "    #   if fit is true, compute the feature importance using the permutation importance criteria \n",
    "    if fit == True:\n",
    "        clf5 = LogisticRegression(random_state=0,penalty='l2',C=1,solver='lbfgs')\n",
    "        trans2 = RFECV(estimator=clf5, step=1, cv=StratifiedKFold(10))\n",
    "        selector2 = trans2.fit(data.X_poly_trainVal,data.y_trainVal)\n",
    "        rfecv_ranking_df = pd.DataFrame(selector2.ranking_,columns=[\"Feature_Ranking\"])\n",
    "        rfecv_ranking_df.to_csv(\"RECV_Feature_Rank_cv10.csv\")\n",
    "       \n",
    "#   if fit is false, read the previously computed SFM feature results. If file has not been created, throw a warning message\n",
    "    if fit == False:\n",
    "        try:\n",
    "            recv_df = pd.read_csv(\"RECV_Feature_Rank_cv10.csv\")\n",
    "        \n",
    "        except:\n",
    "            print(\"Select from model feature selection has not yet been conducted. Please set 'fit=True'\")\n",
    "            return\n",
    "        \n",
    "    print(f\"The filtered data set has been reduced to {len(list(recv_df[recv_df['Feature_Ranking'] == 1].index))} from {x_data.shape[1]} features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cv_rfecv(x_data,y_data,reg_list,cv_folds):\n",
    "\n",
    "#   read the mutual information feature importance from csv. If no file exists throw an error\n",
    "    try:\n",
    "        recv_df = pd.read_csv(\"RECV_Feature_Rank_cv10.csv\")\n",
    "        \n",
    "    except:\n",
    "        print(\"Select from model feature selection has not been conducted, please run function 'feat_select_variance_threshold' first.\")\n",
    "        return\n",
    "\n",
    "    recv_cols_keep = list(recv_df[recv_df['Feature_Ranking'] == 1].index)\n",
    "    X_poly_filter = x_data.loc[:,recv_cols_keep]\n",
    "    \n",
    "    results_rfecv = pd.DataFrame()\n",
    "    log_reg_params = {\"penalty\": ['l2'], 'C': reg_list, 'drop':['Fixed']}\n",
    "    grid = ParameterGrid(log_reg_params)\n",
    "    count = 1\n",
    "\n",
    "#   iterate over the parameter list\n",
    "    for params in grid:\n",
    "\n",
    "        clear_output()\n",
    "        print(f\"Test {count} of {len(grid)}...\")\n",
    "\n",
    "#       conduct cross validation with the filtered features\n",
    "        total_results_df, results_df = cv_test(X_poly_filter, y_data, folds=cv_folds, regularize=params['C'], penalty = params['penalty'], drop = params['drop'])\n",
    "        results_rfecv = pd.concat([results_rfecv,results_df])\n",
    "        count += 1\n",
    "\n",
    "    display(results_rfecv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cv_rfecv_learning_curve(x_data,y_data,folds,penalty,step):\n",
    "\n",
    "    recv_df = pd.read_csv(\"RECV_Feature_Rank_cv10.csv\")\n",
    "    recv_cols_keep = list(recv_df[recv_df['Feature_Ranking'] == 1].index)\n",
    "    X_poly_filter = x_data.loc[:,recv_cols_keep]\n",
    "    \n",
    "    learning_stats = []\n",
    "    clf1 = LogisticRegression(random_state=0,penalty='l2',C=penalty,solver='lbfgs')\n",
    "\n",
    "    for size in np.arange(step,1.00,step):\n",
    "        clear_output()\n",
    "        display(\"Cross Validation Test {} of {}...\".format(int(round(size*100)), int(1/step))) \n",
    "\n",
    "        X_cv, X_unused, y_cv, y_unused = train_test_split(X_poly_filter, y_data, test_size=1-size, random_state=42, stratify=y_data)\n",
    "        scores = cross_validate(clf1, X_cv, y_cv, cv=folds, scoring='f1', return_train_score=True)\n",
    "        learning_stats.append({'data_size':size*100,'train_score':np.mean(scores['train_score']),'test_score':np.mean(scores['test_score'])})\n",
    "\n",
    "    clear_output()\n",
    "    display(\"Cross Validation Test {} of {}...\".format(int(1/step), int(1/step)))\n",
    "    scores = cross_validate(clf1, X_poly_filter, y_data, cv=folds, scoring='f1', return_train_score=True)\n",
    "    learning_stats.append({'data_size':100,'train_score':np.mean(scores['train_score']),'test_score':np.mean(scores['test_score'])})\n",
    "    \n",
    "    learning_df = pd.DataFrame(learning_stats)\n",
    "\n",
    "    fig, ax1 = plt.subplots()\n",
    "    fig.set_size_inches(15,10)\n",
    "\n",
    "    ax1.plot(learning_df['data_size'],learning_df['train_score'],color='red')\n",
    "    ax1.plot(learning_df['data_size'],learning_df['test_score'],color='green')\n",
    "    ax1.set_ylabel('F1 Score')\n",
    "    ax1.set_xlabel('Data Set Percentage of Total Training Validation Set')\n",
    "    ax1.set_title('Learning Curve')\n",
    "#     ssax1.xlim(0,100)\n",
    "\n",
    "    fig.legend(['Train Score','Validation Score'],loc=(0.75, 0.82), prop={'size':12})\n",
    "\n",
    "    ax1.grid()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cv_test_resampling(x_data,y_data,folds,regularize, ratio_list,method):\n",
    "    \n",
    "    strat_scores = []\n",
    "    skf = StratifiedKFold(n_splits=folds, random_state=0, shuffle=False)\n",
    "    skf.get_n_splits(x_data, y_data)\n",
    "    count = 1\n",
    "\n",
    "    for split in ratio_list:\n",
    "        clear_output()\n",
    "        print(\"Test {} of {}...\".format(count, len(ratio_list)))\n",
    "        count += 1\n",
    "            \n",
    "        for train_index, test_index in skf.split(x_data, y_data):\n",
    "\n",
    "    # #       recombine the data so the features and target values are correctly aligned for filtering  \n",
    "    #         merge_df = pd.DataFrame(x_data.iloc[train_index])\n",
    "    #         merge_df['Class'] = y_data.iloc[train_index]\n",
    "\n",
    "    # #       identify the positive cases of the fraud and isolate the same number of non positive cases\n",
    "    #         merge_fraud_df = merge_df[merge_df['Class'] == 1]\n",
    "    #         merge_non_fraud_df = merge_df[merge_df['Class'] == 0].iloc[0:len(merge_fraud_df),:]\n",
    "\n",
    "    # #       recombine the data and shuffle the data set  \n",
    "    #         under_sample_df = pd.concat([merge_fraud_df, merge_non_fraud_df])\n",
    "    #         undersample_shuffle = under_sample_df.sample(frac=1, random_state=0)\n",
    "\n",
    "    # #       separate the features and target from the undersampled data set  \n",
    "    #         under_sample_feats = undersample_shuffle[undersample_shuffle.columns[:-1]]\n",
    "    #         under_sample_target = undersample_shuffle[undersample_shuffle.columns[-1]]\n",
    "            \n",
    "            if method == 'under':\n",
    "                undersample = RandomUnderSampler(sampling_strategy=split,random_state=42)\n",
    "                re_sample_feats, re_sample_target = undersample.fit_resample(x_data.iloc[train_index], y_data.iloc[train_index])\n",
    "                \n",
    "            elif method == 'over':\n",
    "                sm = SMOTE(sampling_strategy=split,random_state=42)\n",
    "                re_sample_feats, re_sample_target = sm.fit_resample(x_data.iloc[train_index], y_data.iloc[train_index])\n",
    "                \n",
    "\n",
    "    #       fit the model based on the undersampled data\n",
    "            clf2 = LogisticRegression(random_state=0,penalty='l2',C=regularize,solver='lbfgs').fit(re_sample_feats,re_sample_target)\n",
    "\n",
    "    #       predict based on the original data set  \n",
    "            y_prob_train = clf2.predict_proba(re_sample_feats)\n",
    "            y_prob_test = clf2.predict_proba(x_data.iloc[test_index])\n",
    "\n",
    "            y_pred_train = np.array(y_prob_train[:,1] > 0.5).astype(int)\n",
    "            y_pred_test = np.array(y_prob_test[:,1] > 0.5).astype(int)\n",
    "\n",
    "    #       compute the recall and precision scores for the undersampled data set\n",
    "            train_result = precision_recall_fscore_support(re_sample_target, y_pred_train, average='binary',pos_label=1)\n",
    "            test_result = precision_recall_fscore_support(y_data.iloc[test_index], y_pred_test, average='binary',pos_label=1)\n",
    "\n",
    "            strat_scores.append({'Train_Score':train_result[2],'Train_Recall':train_result[1], 'Train_Precision':train_result[0],\n",
    "            'Test_Score':test_result[2], 'Test_Recall':test_result[1], 'Test_Precision':test_result[0],'Regularization':regularize,\n",
    "            'Penalty':'l2', 'Ratio':split})\n",
    "        \n",
    "    scores = pd.DataFrame(strat_scores)\n",
    "\n",
    "    return scores,pd.DataFrame(scores.mean()).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cv_test_oversampling(x_data,y_data,folds,regularize, ratio_list,cluster_list):\n",
    "    \n",
    "    strat_scores = []\n",
    "    skf = StratifiedKFold(n_splits=folds, random_state=0, shuffle=False)\n",
    "    skf.get_n_splits(x_data, y_data)\n",
    "    count = 1\n",
    "     \n",
    "    log_reg_params = {\"Ratio\":ratio_list,\"Clusters\": cluster_list}\n",
    "    grid = ParameterGrid(log_reg_params)\n",
    "    count = 1\n",
    "\n",
    "    for params in grid:\n",
    "\n",
    "        clear_output()\n",
    "        print(f\"Over sampling test {count} of {len(grid)}...\") \n",
    "        count += 1\n",
    "            \n",
    "        for train_index, test_index in skf.split(x_data, y_data):\n",
    "                \n",
    "            sm = SMOTE(sampling_strategy=params['Ratio'], k_neighbors=params['Clusters'], random_state=42)\n",
    "            re_sample_feats, re_sample_target = sm.fit_resample(x_data.iloc[train_index], y_data.iloc[train_index])\n",
    "\n",
    "\n",
    "    #       fit the model based on the oversampled data\n",
    "            clf2 = LogisticRegression(random_state=0,penalty='l2',C=regularize,solver='lbfgs').fit(re_sample_feats,re_sample_target)\n",
    "\n",
    "    #       predict based on the original data set  \n",
    "            y_prob_train = clf2.predict_proba(re_sample_feats)\n",
    "            y_prob_test = clf2.predict_proba(x_data.iloc[test_index])\n",
    "\n",
    "            y_pred_train = np.array(y_prob_train[:,1] > 0.5).astype(int)\n",
    "            y_pred_test = np.array(y_prob_test[:,1] > 0.5).astype(int)\n",
    "\n",
    "    #       compute the recall and precision scores for the undersampled data set\n",
    "            train_result = precision_recall_fscore_support(re_sample_target, y_pred_train, average='binary',pos_label=1)\n",
    "            test_result = precision_recall_fscore_support(y_data.iloc[test_index], y_pred_test, average='binary',pos_label=1)\n",
    "\n",
    "            strat_scores.append({'Train_Score':train_result[2],'Train_Recall':train_result[1], 'Train_Precision':train_result[0],\n",
    "            'Test_Score':test_result[2], 'Test_Recall':test_result[1], 'Test_Precision':test_result[0],'Regularization':regularize,\n",
    "            'Penalty':'l2', 'Ratio':params['Ratio'],'Clusters':params['Clusters']})\n",
    "        \n",
    "    scores = pd.DataFrame(strat_scores)\n",
    "\n",
    "    return scores,pd.DataFrame(scores.mean()).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def oos_test(x_train,y_train,x_test,y_test,folds,regularize, ratio,clusters):\n",
    "                 \n",
    "    sm = SMOTE(sampling_strategy=ratio, k_neighbors=clusters, random_state=42)\n",
    "    re_sample_feats, re_sample_target = sm.fit_resample(x_train, y_train)\n",
    "    strat_scores = []\n",
    "    \n",
    "#     pdb.set_trace()\n",
    "\n",
    "#       fit the model based on the over sampled data\n",
    "    clf2 = LogisticRegression(random_state=0,penalty='l2',C=regularize,solver='lbfgs').fit(re_sample_feats,re_sample_target)\n",
    "\n",
    "#       predict based on the original data set  \n",
    "    y_prob_train = clf2.predict_proba(re_sample_feats)\n",
    "    y_prob_test = clf2.predict_proba(x_test)\n",
    "\n",
    "    y_pred_train = np.array(y_prob_train[:,1] > 0.5).astype(int)\n",
    "    y_pred_test = np.array(y_prob_test[:,1] > 0.5).astype(int)\n",
    "\n",
    "#       compute the recall and precision scores for the undersampled data set\n",
    "    train_result = precision_recall_fscore_support(re_sample_target, y_pred_train, average='binary',pos_label=1)\n",
    "    test_result = precision_recall_fscore_support(y_test, y_pred_test, average='binary',pos_label=1)\n",
    "\n",
    "    strat_scores.append({'Train_Score':train_result[2],'Train_Recall':train_result[1], 'Train_Precision':train_result[0],\n",
    "    'Test_Score':test_result[2], 'Test_Recall':test_result[1], 'Test_Precision':test_result[0],'Regularization':regularize,\n",
    "    'Penalty':'l2', 'Ratio':ratio,'Clusters':clusters})\n",
    "        \n",
    "    scores = pd.DataFrame(strat_scores)\n",
    "\n",
    "    return scores,pd.DataFrame(scores.mean()).T,confusion_matrix(y_test,y_pred_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
